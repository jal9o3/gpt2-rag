{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPfUaHAZj6xMuxZps8TfeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jal9o3/gpt2-rag/blob/main/Webpage_RAG_Query_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 RAG Experiments\n",
        "This notebook walks through my experimentation with utilizing a RAG approach with the open source GPT-2 model."
      ],
      "metadata": {
        "id": "vVUYZ2qFMTD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Code\n",
        "This is where we use requests and BeautifulSoup to retrieve the page content.\n",
        "\n",
        "We also parse sentences using NLTK."
      ],
      "metadata": {
        "id": "M3ygvbkNyxu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "XeYtcd1Cy6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "1PPik9UAzsgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get('https://en.wikipedia.org/wiki/Retrieval-augmented_generation')\n",
        "r.text"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UO2YCKrAy_L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "#print(soup.prettify())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tLTlP_dqzv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_paragraphs = soup.find_all(\"p\")"
      ],
      "metadata": {
        "id": "_1eMDrQICELV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_paragraphs)"
      ],
      "metadata": {
        "id": "mxpO06_NChIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = \"\"\n",
        "for raw_paragraph in raw_paragraphs:\n",
        "    paragraphs += \"\\n\" + raw_paragraph.text\n",
        "print(paragraphs)"
      ],
      "metadata": {
        "id": "GJFGcXazCNQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "5cU7vv6X5_Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ABHTy9F36R8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(paragraphs)\n",
        "print(sentences)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KwkQMXwF6BZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Code\n",
        "Here we generate embeddings using a sentence transformer model from Hugging Face."
      ],
      "metadata": {
        "id": "oO0dYRi806Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C_TlZ6hD08vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "BBBgNU8D1BdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "id": "6lL1bD8O2BZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pinecone VectorDB Code\n",
        "Here we upsert the embeddings to PineconeDB and run a similarity search."
      ],
      "metadata": {
        "id": "8V9jlfBX2MFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get('PC_API_KEY')"
      ],
      "metadata": {
        "id": "m85WHqKr2XiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"pinecone-client[grpc]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "BHJaLr5T3JNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "JDlHPK653mTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pc.delete_index(\"webpage-index\")"
      ],
      "metadata": {
        "id": "it4Nfy1TGINa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"webpage-index\"\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384, # dimensions of embeddings\n",
        "        metric=\"cosine\", # used by the embedding model\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "Aczk6DnC4Dru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name)\n",
        "\n",
        "vectors = []\n",
        "for i, embedding in enumerate(embeddings):\n",
        "  vectors.append({\"id\": f\"vec{i}\", \"values\": embedding})\n",
        "\n",
        "index.upsert(\n",
        "    vectors,\n",
        "    namespace=\"webpage-1\"\n",
        ")"
      ],
      "metadata": {
        "id": "Cq5VC4qY4eC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "aSVNkdsA5YeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_stats = index.describe_index_stats()\n",
        "# Account for upsert delay by checking the number of vectors in DB\n",
        "len(embeddings) == index_stats['namespaces']['webpage-1']['vector_count']"
      ],
      "metadata": {
        "id": "7F_1A4dT5h5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate an embedding of the prompt\n",
        "prompt = \"What is Retrieval Augmented Generation?\"\n",
        "vector = model.encode([prompt])[0]"
      ],
      "metadata": {
        "id": "s4IdoViY6P2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J0qUF0jU6pqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_results = index.query(\n",
        "    namespace=\"webpage-1\",\n",
        "    vector=vector,\n",
        "    top_k=2,\n",
        "    include_values=True\n",
        ")\n",
        "\n",
        "#print(query_results)\n",
        "\n",
        "for query_match in query_results['matches']:\n",
        "  print(query_match['id'])"
      ],
      "metadata": {
        "id": "26qskKGE58t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse result indexes using regex (temporary ;))\n",
        "import re\n",
        "id_nums = []\n",
        "for query_match in query_results['matches']:\n",
        "  id_nums.append(int(re.findall(r'\\d+', query_match['id'])[0]))\n",
        "print(id_nums)"
      ],
      "metadata": {
        "id": "p9HYW-DY8S2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct background info for GPT-2\n",
        "background = sentences[0]\n",
        "for id in id_nums:\n",
        "  if id > 0:\n",
        "    background += \" \" + sentences[id]\n",
        "print(background)"
      ],
      "metadata": {
        "id": "T7iU60Ch9hGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Code\n",
        "This is where we use the GPT-2 model from Hugging Face."
      ],
      "metadata": {
        "id": "r75vKrCZymDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk3kAhKUxGQQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extended_prompt = f\"\"\"\n",
        "{background.strip()} {prompt}\n",
        "\"\"\"\n",
        "print(extended_prompt)"
      ],
      "metadata": {
        "id": "Yt2ASUSy_upW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "input_ids = tokenizer(extended_prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=100,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]"
      ],
      "metadata": {
        "id": "WAAauigzxSBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gen_text)"
      ],
      "metadata": {
        "id": "6Zo1nasgxVDD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}